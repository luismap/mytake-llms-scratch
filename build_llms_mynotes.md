* overview of AI ![alt text](images/ai_overview.png)
* process of training an LLM ![alt text](images/train_llm.png)
* simplified transformer arch p.12 ![alt text](images/simplified_trans.png)
* overview of encoder decoder modules p.14 ![alt text](images/enc_dec.png)
* pretraining dataset of gpt3 p.16 ![alt text](images/gpt3_pre_ds.png)
* transformer arch and tasks ![alt text](images/trans_task.png)
* decoder only approach ![alt text](images/decoder_only.png)
* stages for building an llm p.20 ![alt text](images/build_stages_llms.png)
* embedding models ![alt text](images/embeddings.png)
* tokenization process ![alt text](images/tokenization_proc.png)
* vocabulary ![alt text](images/vocab.png)
* llms predicts next token ![alt text](images/predict_n_tok.png)
* embeddings layer perform lookup ops ![alt text](images/embedding_lookup.png)
* a note about embedding layer p.58 ![alt text](images/embedding_note.png)
* input embeddings ![alt text](images/input_embed.png)
* ![alt text](images/embedding_pipeline.png)


# notes
* **about embeddings**: while word embeddings are the most common form of text embedding, there are also
embeddings for sentences, paragraphs, or whole documents. Sentence or paragraph
embeddings are popular choices for retrieval-augmented generation. Retrieval-augmented
generation combines generation (like producing text) with retrieval (like searching an
external knowledge base) to pull relevant information when generating text, which is a
technique that is beyond the scope of this book